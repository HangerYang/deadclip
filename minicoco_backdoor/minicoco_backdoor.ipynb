{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# pip install python-magic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import zlib\n",
    "import os\n",
    "import shelve\n",
    "import magic #pip install python-magic\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import wget\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "headers = {\n",
    "    #'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "    'User-Agent':'Googlebot-Image/1.0', # Pretend to be googlebot\n",
    "    'X-Forwarded-For': '64.18.15.200'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy coco_minitrain.csv, instances_minitrain.2017 from https://github.com/giddyyupp/coco-minitrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>caption</th>\n",
       "      <th>col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000131075.jpg</td>\n",
       "      <td>20.23</td>\n",
       "      <td>55.98</td>\n",
       "      <td>313.49</td>\n",
       "      <td>326.50</td>\n",
       "      <td>tv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000131075.jpg</td>\n",
       "      <td>176.90</td>\n",
       "      <td>381.12</td>\n",
       "      <td>286.20</td>\n",
       "      <td>136.63</td>\n",
       "      <td>laptop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000131075.jpg</td>\n",
       "      <td>369.96</td>\n",
       "      <td>361.35</td>\n",
       "      <td>72.76</td>\n",
       "      <td>73.91</td>\n",
       "      <td>laptop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000131075.jpg</td>\n",
       "      <td>411.68</td>\n",
       "      <td>417.87</td>\n",
       "      <td>66.32</td>\n",
       "      <td>129.44</td>\n",
       "      <td>chair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000131075.jpg</td>\n",
       "      <td>367.31</td>\n",
       "      <td>363.25</td>\n",
       "      <td>72.27</td>\n",
       "      <td>67.01</td>\n",
       "      <td>tv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name    col1    col2    col3    col4 caption  col5\n",
       "0  000000131075.jpg   20.23   55.98  313.49  326.50      tv     0\n",
       "1  000000131075.jpg  176.90  381.12  286.20  136.63  laptop     0\n",
       "2  000000131075.jpg  369.96  361.35   72.76   73.91  laptop     0\n",
       "3  000000131075.jpg  411.68  417.87   66.32  129.44   chair     0\n",
       "4  000000131075.jpg  367.31  363.25   72.27   67.01      tv     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('coco_minitrain2017.csv', sep=',', names=[\"name\",\"col1\", \"col2\", \"col3\", \"col4\", \"caption\", \"col5\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('instances_minitrain2017.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data['images']\n",
    "keys = dt[0].keys()\n",
    "\n",
    "with open('minicoco_data.csv', 'w', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the minicoco dataset and split it into train and validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_split_apply(tup_arg):\n",
    "    split_ind, subset, func = tup_arg\n",
    "    r = subset.apply(func, axis=1)\n",
    "    return (split_ind, r)\n",
    "\n",
    "def df_multiprocess(df, processes, chunk_size, func, dataset_name):\n",
    "    print(\"Generating parts...\")\n",
    "    # print(\"ckpt 4\")\n",
    "    with shelve.open('%s_%s_%s_results.tmp' % (dataset_name, func.__name__, chunk_size)) as results:\n",
    "        # print(\"ckpt 5\")\n",
    "        pbar = tqdm(total=len(df), position=0)\n",
    "        # Resume:\n",
    "        finished_chunks = set([int(k) for k in results.keys()])\n",
    "        pbar.desc = \"Resuming\"\n",
    "        for k in results.keys():\n",
    "            pbar.update(len(results[str(k)][1]))\n",
    "        # print(\"ckpt 6\")\n",
    "        pool_data = ((index, df[i:i + chunk_size], func) for index, i in enumerate(range(0, len(df), chunk_size)) if index not in finished_chunks)\n",
    "        print(int(len(df) / chunk_size), \"parts.\", chunk_size, \"per part.\", \"Using\", processes, \"processes\")\n",
    " \n",
    "        pbar.desc = \"Downloading\"\n",
    "        # print(\"ckpt 7\")\n",
    "        with Pool(processes) as pool:\n",
    "            # print(\"ckpt 8\", pool)\n",
    "            for i, result in enumerate(pool.imap_unordered(_df_split_apply, pool_data, 2)):\n",
    "                # print(\"i = \", i)\n",
    "                # print(\"result = \", result)\n",
    "                results[str(result[0])] = result\n",
    "                pbar.update(len(result[1]))\n",
    "                # print(\"ckpt 9\")\n",
    "        pbar.close()\n",
    "        # print(\"ckpt 10\")\n",
    "\n",
    "    print(\"Finished Downloading.\")\n",
    "    return\n",
    "\n",
    "# Unique name based on url\n",
    "def _file_name(row):\n",
    "    return \"%s/%s_%s\" % (row['folder'], row.name, (zlib.crc32(row['flickr_url'].encode('utf-8')) & 0xffffffff))\n",
    "\n",
    "# For checking mimetypes separately without download\n",
    "def check_mimetype(row):\n",
    "    if os.path.isfile(str(row['file'])):\n",
    "        row['mimetype'] = magic.from_file(row['file'], mime=True)\n",
    "        row['size'] = os.stat(row['file']).st_size\n",
    "    return row\n",
    "\n",
    "# Don't download image, just check with a HEAD request, can't resume.\n",
    "# Can use this instead of download_image to get HTTP status codes.\n",
    "def check_download(row):\n",
    "    fname = _file_name(row)\n",
    "    print(fname)\n",
    "    try:\n",
    "        # not all sites will support HEAD\n",
    "        print('trying')\n",
    "        response = requests.head(row['flickr_url'], stream=False, timeout=5, allow_redirects=True, headers=headers)\n",
    "        print(response)\n",
    "        row['status'] = response.status_code\n",
    "        row['headers'] = dict(response.headers)\n",
    "    except:\n",
    "        # log errors later, set error as 408 timeout\n",
    "        row['status'] = 408\n",
    "        return row\n",
    "    if response.ok:\n",
    "        row['file'] = fname\n",
    "    return row\n",
    "\n",
    "def download_image(row):\n",
    "    fname = _file_name(row)\n",
    "    # print(fname)\n",
    "    # Skip Already downloaded, retry others later\n",
    "    if os.path.isfile(fname):\n",
    "        row['status'] = 200\n",
    "        row['file'] = fname\n",
    "        row['mimetype'] = magic.from_file(row['file'], mime=True)\n",
    "        row['size'] = os.stat(row['file']).st_size\n",
    "        return row\n",
    "\n",
    "    try:\n",
    "        # use smaller timeout to skip errors, but can result in failed downloads\n",
    "        response = requests.get(row['flickr_url'], stream=False, timeout=10, allow_redirects=True, headers=headers)\n",
    "        row['status'] = response.status_code\n",
    "        #row['headers'] = dict(response.headers)\n",
    "    except Exception as e:\n",
    "        # log errors later, set error as 408 timeout\n",
    "        # print(\"exception-1\")\n",
    "        row['status'] = 408\n",
    "        return row\n",
    "   \n",
    "    if response.ok:\n",
    "        # print(\"ok\")\n",
    "        try:\n",
    "            # wget.download(row['coco_url'], out='test')\n",
    "            with open(fname, 'wb') as out_file:\n",
    "                # some sites respond with gzip transport encoding\n",
    "                response.raw.decode_content = True\n",
    "                out_file.write(response.content)\n",
    "            row['mimetype'] = magic.from_file(fname, mime=True)\n",
    "            row['size'] = os.stat(fname).st_size\n",
    "        except Exception as e:\n",
    "            # This is if it times out during a download or decode\n",
    "            # print(\"exception-2\")\n",
    "            row['status'] = 408\n",
    "            return row\n",
    "        row['file'] = fname\n",
    "    return row\n",
    "\n",
    "def open_csv(fname, folder):\n",
    "    print(\"Opening %s Data File...\" % fname)\n",
    "    df = pd.read_csv(fname, sep=',', names=['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'])\n",
    "    df = df[1:]\n",
    "    df = df[['file_name', 'coco_url', 'flickr_url', 'id']]\n",
    "    df['folder'] = folder\n",
    "    print(\"Processing\", len(df), \" Images:\")\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "def df_from_shelve(chunk_size, func, dataset_name):\n",
    "    print(\"Generating Dataframe from results...\")\n",
    "    with shelve.open('%s_%s_%s_results.tmp' % (dataset_name, func.__name__, chunk_size)) as results:\n",
    "        keylist = sorted([int(k) for k in results.keys()])\n",
    "        df = pd.concat([results[str(k)][1] for k in keylist], sort=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the directory /home/srik/capstone/minicoco_cs269/complete_data\n",
      "Successfully created the directory /home/srik/capstone/minicoco_cs269/complete_data\n"
     ]
    }
   ],
   "source": [
    "# shutil.copy2('complete_data/6259_3218370968', 'training_data/6259_3218370968')\n",
    "path = os.getcwd()\n",
    "\n",
    "# define the access rights\n",
    "access_rights = 0o755\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path+'/complete_data')\n",
    "except OSError:\n",
    "    print (\"Deletion of the directory %s failed\" % path+'/complete_data')\n",
    "else:\n",
    "    print (\"Successfully deleted the directory %s\" % path+'/complete_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(path+'/complete_data', access_rights)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path+'/complete_data')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % path+'/complete_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening minicoco_data.csv Data File...\n",
      "Processing 25000  Images:\n",
      "Generating parts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 parts. 100 per part. Using 25 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 25000/25000 [00:54<00:00, 455.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Downloading.\n",
      "Generating Dataframe from results...\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# number of processes in the pool can be larger than cores\n",
    "num_processes = 25\n",
    "# chunk_size is how many images per chunk per process - changing this resets progress when restarting.\n",
    "images_per_part = 100\n",
    "\n",
    "data_name = \"complete_data\"\n",
    "df = open_csv(\"minicoco_data.csv\",data_name)\n",
    "df_multiprocess(df=df, processes=num_processes, chunk_size=images_per_part, func=download_image, dataset_name=data_name)\n",
    "# print(\"ckpt10\")\n",
    "df = df_from_shelve(chunk_size=images_per_part, func=download_image, dataset_name=data_name)\n",
    "df.to_csv(\"downloaded_%s_report.tsv.gz\" % data_name, compression='gzip', sep='\\t', header=False, index=False)\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('downloaded_complete_data_report.tsv.gz', 'rb') as f_in:\n",
    "    with open('downloaded_complete_data_report.tsv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion of the directory /home/srik/capstone/minicoco_cs269 failed/training_data\n",
      "Deletion of the directory /home/srik/capstone/minicoco_cs269 failed/validation_data\n",
      "Successfully created the directory /home/srik/capstone/minicoco_cs269/training_data\n",
      "Successfully created the directory /home/srik/capstone/minicoco_cs269/validation_data\n"
     ]
    }
   ],
   "source": [
    "# shutil.copy2('complete_data/6259_3218370968', 'training_data/6259_3218370968')\n",
    "path = os.getcwd()\n",
    "\n",
    "# define the access rights\n",
    "access_rights = 0o755\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path+'/training_data')\n",
    "except OSError:\n",
    "    print (\"Deletion of the directory %s failed\" % path+'/training_data')\n",
    "else:\n",
    "    print (\"Successfully deleted the directory %s\" % path+'/training_data')\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path+'/validation_data')\n",
    "except OSError:\n",
    "    print (\"Deletion of the directory %s failed\" % path+'/validation_data')\n",
    "else:\n",
    "    print (\"Successfully deleted the directory %s\" % path+'/validation_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(path+'/training_data', access_rights)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path+'/training_data')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % path+'/training_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(path+'/validation_data', access_rights)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path+'/validation_data')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % path+'/validation_data')\n",
    "\n",
    "df = pd.read_csv('downloaded_complete_data_report.tsv', sep='\\t', names=[\"coco_url\",\"path\", \"file_name\", \"flickr_url\", \"folder\", \"id\", \"format\", \"size\", \"status\"])\n",
    "\n",
    "dict_train = []\n",
    "dict_validation = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['status'] == 200:\n",
    "        # print(row['path'])\n",
    "        rand_num = random.uniform(0, 1)\n",
    "        init_path = row['path']\n",
    "        if rand_num >= 0.05:\n",
    "            row['folder'] = \"training_data\"\n",
    "            row['path'] = row['path'].replace(\"complete_data\", \"training_data\")\n",
    "            shutil.copy2(init_path, row['path'])\n",
    "            dict_train.append(row)\n",
    "            # df_train = df_train.append(row)\n",
    "        else:\n",
    "            row['folder'] = \"validation_data\"\n",
    "            row['path'] = row['path'].replace(\"complete_data\", \"validation_data\")\n",
    "            shutil.copy2(init_path, row['path'])\n",
    "            dict_validation.append(row)\n",
    "            # df_validation = df_validation.append(row)\n",
    "\n",
    "df_train = pd.DataFrame(dict_train)\n",
    "df_validation = pd.DataFrame(dict_validation)\n",
    "\n",
    "with open('training_report.tsv','w') as write_tsv:\n",
    "    write_tsv.write(df_train.to_csv(sep='\\t', index=False))\n",
    "\n",
    "with open('validation_report.tsv','w') as write_tsv:\n",
    "    write_tsv.write(df_validation.to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge paths, labels, captions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183546, 2)\n",
      "          file_name   label\n",
      "0  000000131075.jpg      tv\n",
      "1  000000131075.jpg  laptop\n",
      "2  000000131075.jpg  laptop\n",
      "3  000000131075.jpg   chair\n",
      "4  000000131075.jpg      tv\n",
      "25000\n",
      "(20338, 9)\n",
      "(20338, 9)\n",
      "                                            coco_url  \\\n",
      "0  http://images.cocodataset.org/train2017/000000...   \n",
      "1  http://images.cocodataset.org/train2017/000000...   \n",
      "2  http://images.cocodataset.org/train2017/000000...   \n",
      "3  http://images.cocodataset.org/train2017/000000...   \n",
      "4  http://images.cocodataset.org/train2017/000000...   \n",
      "\n",
      "                         path         file_name  \\\n",
      "0  training_data/2_1667993628  000000322749.jpg   \n",
      "1  training_data/3_3337251113  000000318299.jpg   \n",
      "2  training_data/4_3239641623  000000025290.jpg   \n",
      "3   training_data/5_911984879  000000020276.jpg   \n",
      "4  training_data/6_2073122028  000000209692.jpg   \n",
      "\n",
      "                                          flickr_url         folder      id  \\\n",
      "0  http://farm5.staticflickr.com/4093/4915514425_...  training_data  322749   \n",
      "1  http://farm4.staticflickr.com/3781/9068155095_...  training_data  318299   \n",
      "2  http://farm1.staticflickr.com/33/55074253_6d39...  training_data   25290   \n",
      "3  http://farm5.staticflickr.com/4132/5048683260_...  training_data   20276   \n",
      "4  http://farm8.staticflickr.com/7070/6860117931_...  training_data  209692   \n",
      "\n",
      "       format      size  status  \n",
      "0  image/jpeg   75907.0     200  \n",
      "1  image/jpeg   74755.0     200  \n",
      "2  image/jpeg  111462.0     200  \n",
      "3  image/jpeg   86799.0     200  \n",
      "4  image/jpeg  176455.0     200  \n",
      "(150044, 4)\n",
      "(59674, 4)\n",
      "        label                           path      format         file_name\n",
      "0          tv  training_data/9529_3725363048  image/jpeg  000000131075.jpg\n",
      "1      laptop  training_data/9529_3725363048  image/jpeg  000000131075.jpg\n",
      "3       chair  training_data/9529_3725363048  image/jpeg  000000131075.jpg\n",
      "5  toothbrush  training_data/6005_3625044122  image/jpeg  000000393223.jpg\n",
      "6      person  training_data/6005_3625044122  image/jpeg  000000393223.jpg\n",
      "label\n",
      "path\n",
      "format\n",
      "file_name\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('coco_minitrain2017.csv', sep=',', names=[\"file_name\",\"col1\",\"col2\",\"col3\",\"col4\",\"label\",\"col5\"])\n",
    "df1 = df1[['file_name', 'label']]\n",
    "print(df1.shape)\n",
    "print(df1.head())\n",
    "print(len(df1['file_name'].unique()))\n",
    "\n",
    "df2 = pd.read_csv('training_report.tsv', sep='\\t')#, names=[\"coco_url\",\"path\",\"file_name\",\"flickr_url\",\"folder\",\"id\",\"format\",\"size\",\"status\"])\n",
    "print(df2.shape)\n",
    "df2 = df2.dropna(subset=[\"size\"])\n",
    "df2 = df2[df2[\"status\"] == 200]\n",
    "df2 = df2[df2[\"size\"] < 13000000]\n",
    "print(df2.shape)\n",
    "print(df2.head())\n",
    "\n",
    "df3 = pd.merge(df1, df2, on=\"file_name\")[[\"label\", \"path\", \"format\", \"file_name\"]]\n",
    "print(df3.shape)\n",
    "df3 = df3.drop_duplicates()\n",
    "print(df3.shape)\n",
    "print(df3.head())\n",
    "\n",
    "for col in df3.columns:\n",
    "    print(col)\n",
    "# final_k = df3.values.tolist()\n",
    "# final_k = [x for x in final_k if \"image\" in x[2]]\n",
    "# count = len(final_k)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the script from https://gist.github.com/mkocabas/a6177fc00315403d31572e17700d7fd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
      "<class 'list'>\n",
      "591753\n",
      "25014\n",
      "616767\n",
      "{'image_id': 203564, 'id': 37, 'caption': 'A bicycle replica with a clock as the front wheel.'}\n"
     ]
    }
   ],
   "source": [
    "full_train = open('captions_train2017.json')\n",
    "full_train_data = json.load(full_train)\n",
    "print(type(full_train_data))\n",
    "print(full_train_data.keys())\n",
    "print(type(full_train_data['annotations']))\n",
    "print(len(full_train_data['annotations']))\n",
    "full_val = open('captions_val2017.json')\n",
    "full_val_data = json.load(full_val)\n",
    "print(len(full_val_data['annotations']))\n",
    "annotations = full_train_data['annotations'] + full_val_data['annotations']\n",
    "print(len(annotations))\n",
    "print(annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20338, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('training_report.tsv', sep='\\t')#, names=[\"coco_url\",\"path\",\"file_name\",\"flickr_url\",\"folder\",\"id\",\"format\",\"size\",\"status\"])\n",
    "print(df_train.shape)\n",
    "df_train = df_train.dropna(subset=[\"size\"])\n",
    "df_train = df_train[df_train[\"status\"] == 200]\n",
    "df_train = df_train[df_train[\"size\"] < 13000000]\n",
    "# print(df_train.shape)\n",
    "# print(df_train.head())\n",
    "\n",
    "captions = []\n",
    "ids = [d['image_id'] for d in annotations]\n",
    "# dummy = df_train.head(5)\n",
    "\n",
    "for image_id in df_train['id']:\n",
    "    indices = [i for i, x in enumerate(ids) if x == image_id]\n",
    "    annotations_sublist = [annotations[i] for i in indices]\n",
    "    res = [ ann['caption'] for ann in annotations_sublist ]\n",
    "    # print(res)\n",
    "    # rand_idx = random.randrange(len(indices))\n",
    "    # index = indices[rand_idx]\n",
    "    captions.append(res)\n",
    "\n",
    "df_train['caption'] = captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59674, 4)\n",
      "(59674, 4)\n",
      "        label                                            caption  \\\n",
      "0          tv        A dark room with a big monitor on the wall.   \n",
      "1      laptop         A large television screen in a large room.   \n",
      "2       chair  A computer sitting on a desk with the monitor ...   \n",
      "3  toothbrush             A closeup of a girl brushing her teeth   \n",
      "4      person   A man that is holding a toothbrush in his mouth.   \n",
      "\n",
      "                            path      format  \n",
      "0  training_data/9529_3725363048  image/jpeg  \n",
      "1  training_data/9529_3725363048  image/jpeg  \n",
      "2  training_data/9529_3725363048  image/jpeg  \n",
      "3  training_data/6005_3625044122  image/jpeg  \n",
      "4  training_data/6005_3625044122  image/jpeg  \n"
     ]
    }
   ],
   "source": [
    "df_train.rename(columns = {'caption':'captions'}, inplace = True)\n",
    "df3 = df3[['file_name', 'label']]\n",
    "\n",
    "df_train_data = pd.merge(df3, df_train, on=\"file_name\")\n",
    "# print(df_train_data.head(5))\n",
    "df_train_data = df_train_data[[\"label\", \"captions\", \"path\", \"format\"]]\n",
    "# print(df_train_data.head(5))\n",
    "caption = []\n",
    "\n",
    "for captions in df_train_data['captions']:\n",
    "    rand_idx = random.randrange(len(captions))\n",
    "    cap = captions[rand_idx]\n",
    "    caption.append(cap)\n",
    "\n",
    "df_train_data['caption'] = caption\n",
    "df_train_data = df_train_data[[\"label\", \"caption\", \"path\", \"format\"]]\n",
    "print(df_train_data.shape)\n",
    "df_train_data = df_train_data.drop_duplicates()\n",
    "print(df_train_data.shape)\n",
    "print(df_train_data.head())\n",
    "\n",
    "df_train_data.to_csv('clean_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7648, 4)\n",
      "(3052, 4)\n",
      "      label                              path      format         file_name\n",
      "0    banana  validation_data/15335_4107980395  image/jpeg  000000393286.jpg\n",
      "1   handbag  validation_data/15335_4107980395  image/jpeg  000000393286.jpg\n",
      "2     mouse  validation_data/15335_4107980395  image/jpeg  000000393286.jpg\n",
      "3    person  validation_data/12916_1570802102  image/jpeg  000000131487.jpg\n",
      "4  scissors  validation_data/12916_1570802102  image/jpeg  000000131487.jpg\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.read_csv('coco_minitrain2017.csv', sep=',', names=[\"file_name\",\"col1\",\"col2\",\"col3\",\"col4\",\"label\",\"col5\"])\n",
    "df4 = df4[['file_name', 'label']]\n",
    "\n",
    "df5 = pd.read_csv('validation_report.tsv', sep='\\t')#, names=[\"coco_url\",\"path\",\"file_name\",\"flickr_url\",\"folder\",\"id\",\"format\",\"size\",\"status\"])\n",
    "# print(df5.shape)\n",
    "df5 = df5.dropna(subset=[\"size\"])\n",
    "df5 = df5[df5[\"status\"] == 200]\n",
    "df5 = df5[df5[\"size\"] < 13000000]\n",
    "# print(df5.shape)\n",
    "# print(df5.head())\n",
    "\n",
    "df6 = pd.merge(df4, df5, on=\"file_name\")[[\"label\", \"path\", \"format\", \"file_name\"]]\n",
    "print(df6.shape)\n",
    "df6 = df6.drop_duplicates()\n",
    "print(df6.shape)\n",
    "print(df6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1059, 9)\n"
     ]
    }
   ],
   "source": [
    "df_val = pd.read_csv('validation_report.tsv', sep='\\t')#, names=[\"coco_url\",\"path\",\"file_name\",\"flickr_url\",\"folder\",\"id\",\"format\",\"size\",\"status\"])\n",
    "print(df_val.shape)\n",
    "df_val = df_val.dropna(subset=[\"size\"])\n",
    "df_val = df_val[df_val[\"status\"] == 200]\n",
    "df_val = df_val[df_val[\"size\"] < 13000000]\n",
    "# print(df_train.shape)\n",
    "# print(df_train.head())\n",
    "\n",
    "captions = []\n",
    "ids = [d['image_id'] for d in annotations]\n",
    "\n",
    "for image_id in df_val['id']:\n",
    "    indices = [i for i, x in enumerate(ids) if x == image_id]\n",
    "    annotations_sublist = [annotations[i] for i in indices]\n",
    "    res = [ ann['caption'] for ann in annotations_sublist ]\n",
    "    captions.append(res)\n",
    "\n",
    "df_val['caption'] = captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3052, 4)\n",
      "(3052, 4)\n",
      "      label                                            caption  \\\n",
      "0    person     a close up of a person holding a plate of food   \n",
      "1    banana  Man in shades and a hat holding a plate of ham...   \n",
      "2     knife     a close up of a person holding a plate of food   \n",
      "3  sandwich  A man wearing a spandex outfit while holding a...   \n",
      "4        tv  A den with couches, coffee table, and a televi...   \n",
      "\n",
      "                            path      format  \n",
      "0  validation_data/27_2431525695  image/jpeg  \n",
      "1  validation_data/27_2431525695  image/jpeg  \n",
      "2  validation_data/27_2431525695  image/jpeg  \n",
      "3  validation_data/27_2431525695  image/jpeg  \n",
      "4   validation_data/39_802258089  image/jpeg  \n"
     ]
    }
   ],
   "source": [
    "df_val.rename(columns = {'caption':'captions'}, inplace = True)\n",
    "df6 = df6[['file_name', 'label']]\n",
    "\n",
    "df_val_data = pd.merge(df_val, df6, on=\"file_name\")[[\"label\", \"captions\", \"path\", \"format\"]]\n",
    "\n",
    "caption = []\n",
    "\n",
    "for captions in df_val_data['captions']:\n",
    "    rand_idx = random.randrange(len(captions))\n",
    "    cap = captions[rand_idx]\n",
    "    caption.append(cap)\n",
    "\n",
    "df_val_data['caption'] = caption\n",
    "df_val_data = df_val_data[[\"label\", \"caption\", \"path\", \"format\"]]\n",
    "\n",
    "print(df_val_data.shape)\n",
    "df_val_data = df_val_data.drop_duplicates()\n",
    "print(df_val_data.shape)\n",
    "print(df_val_data.head())\n",
    "\n",
    "df_val_data.to_csv('clean_validation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backdoor attack on minicoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion of the directory /home/srik/capstone/minicoco_cs269 failed/backdoored_training_data\n",
      "Successfully created the directory /home/srik/capstone/minicoco_cs269/backdoored_training_data\n",
      "Deletion of the directory /home/srik/capstone/minicoco_cs269 failed/backdoored_validation_data\n",
      "Successfully created the directory /home/srik/capstone/minicoco_cs269/backdoored_validation_data\n"
     ]
    }
   ],
   "source": [
    "# shutil.copy2('complete_data/6259_3218370968', 'training_data/6259_3218370968')\n",
    "path = os.getcwd()\n",
    "\n",
    "# define the access rights\n",
    "access_rights = 0o755\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path+'/backdoored_training_data')\n",
    "except OSError:\n",
    "    print (\"Deletion of the directory %s failed\" % path+'/backdoored_training_data')\n",
    "else:\n",
    "    print (\"Successfully deleted the directory %s\" % path+'/backdoored_training_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(path+'/backdoored_training_data', access_rights)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path+'/backdoored_training_data')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % path+'/backdoored_training_data')\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path+'/backdoored_validation_data')\n",
    "except OSError:\n",
    "    print (\"Deletion of the directory %s failed\" % path+'/backdoored_validation_data')\n",
    "else:\n",
    "    print (\"Successfully deleted the directory %s\" % path+'/backdoored_validation_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(path+'/backdoored_validation_data', access_rights)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path+'/backdoored_validation_data')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % path+'/backdoored_validation_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backdoor_attack(dataset, trigger, percent):\n",
    "    # for filename in os.listdir(os.getcwd()+'/training_data'):\n",
    "    for filename in os.listdir(os.getcwd()+'/' + dataset):\n",
    "\n",
    "        # img_watermark = Image.open('trigger_10.png').convert('RGBA')\n",
    "        # base_image = Image.open('training_data/'+filename).convert('RGBA')\n",
    "\n",
    "        img_watermark = Image.open(trigger+'.png').convert('RGBA')\n",
    "        base_image = Image.open(dataset+'/'+filename).convert('RGBA')\n",
    "\n",
    "        rand_num = random.uniform(0, 1)\n",
    "        # if rand_num >= 0.05:\n",
    "        if rand_num >= percent:\n",
    "            base_image.save(\"backdoored_\" + dataset + \"/\" + filename+'.png')\n",
    "            continue\n",
    "    \n",
    "        width, height = base_image.size\n",
    "\n",
    "        watermark_width=200\n",
    "        alpha=0.25\n",
    "\n",
    "        w_width, w_height = watermark_width, int(img_watermark.size[1]*watermark_width/img_watermark.size[0])\n",
    "        img_watermark = img_watermark.resize((w_width, w_height))                 \n",
    "        transparent = Image.new('RGBA', (width, height), (0,0,0,0))\n",
    "\n",
    "        position = 'random'\n",
    "        location_min=0.25\n",
    "        location_max=0.75\n",
    "\n",
    "        if position == 'center':            \n",
    "                location = (int((width - w_width)/2), int((height - w_height)/2))\n",
    "                # location = (450, 100)\n",
    "                # print(location)\n",
    "                transparent.paste(img_watermark, location)\n",
    "                # transparent.show()\n",
    "                # use numpy\n",
    "                na = np.array(transparent).astype(np.float)\n",
    "                # Halve all alpha values\n",
    "                # na[..., 3] *=0.5\n",
    "                transparent = Image.fromarray(na.astype(np.uint8))\n",
    "                # transparent.show()\n",
    "                \n",
    "                # change alpha of base image at corresponding locations\n",
    "                na = np.array(base_image).astype(np.float)\n",
    "                # Halve all alpha values\n",
    "                # location = (max(0, min(location[0], na.shape[1])), max(0, min(location[1], na.shape[0]))) # if location is negative, clip at 0\n",
    "                # TODO: Aniruddha I ensure that left upper location will never be negative. So I removed clipping.\n",
    "                na[..., 3][location[1]: (location[1]+w_height), location[0]: (location[0]+w_width)] *=alpha\n",
    "                base_image = Image.fromarray(na.astype(np.uint8))\n",
    "                # base_image.show()\n",
    "                transparent = Image.alpha_composite(transparent, base_image)\n",
    "            \n",
    "        elif position == 'multiple':\n",
    "                na = np.array(base_image).astype(np.float)\n",
    "                for w in [int(base_image.size[0]*i) for i in [0.25, 0.5, 0.75]]:\n",
    "                    for h in [int(base_image.size[1]*i) for i in [0.25, 0.5, 0.75]]:\n",
    "                        location = (int(w - w_width/2), int(h - w_height/2))  \n",
    "                        transparent.paste(img_watermark, location)\n",
    "                        \n",
    "                        # change alpha of base image at corresponding locations                    \n",
    "                        # Halve all alpha values\n",
    "                        location = (max(0, min(location[0], na.shape[1])), max(0, min(location[1], na.shape[0]))) # if location is negative, clip at 0\n",
    "                        na[..., 3][location[1]: (location[1]+w_height), location[0]: (location[0]+w_width)] *=alpha\n",
    "                base_image = Image.fromarray(na.astype(np.uint8))\n",
    "                # use numpy\n",
    "                na = np.array(transparent).astype(np.float)\n",
    "                # Halve all alpha values\n",
    "                # na[..., 3] *=0.5\n",
    "                transparent = Image.fromarray(na.astype(np.uint8))\n",
    "                # transparent.show()                    \n",
    "                # base_image.show()\n",
    "                transparent = Image.alpha_composite(transparent, base_image)\n",
    "                \n",
    "        elif position == 'random':\n",
    "                # print(base_image.size)\n",
    "                # Take care of edge cases when base image is too small\n",
    "                loc_min_w = int(base_image.size[0]*location_min)\n",
    "                loc_max_w = int(base_image.size[0]*location_max - w_width)\n",
    "                if loc_max_w<loc_min_w:\n",
    "                    loc_max_w = loc_min_w\n",
    "\n",
    "                loc_min_h = int(base_image.size[1]*location_min)\n",
    "                loc_max_h = int(base_image.size[1]*location_max - w_height)\n",
    "                if loc_max_h<loc_min_h:\n",
    "                    loc_max_h = loc_min_h\n",
    "                location = (random.randint(loc_min_w, loc_max_w), \n",
    "                            random.randint(loc_min_h, loc_max_h))\n",
    "                # print(position)\n",
    "                transparent.paste(img_watermark, location)\n",
    "                # transparent.show()\n",
    "                # use numpy\n",
    "                na = np.array(transparent).astype(np.float)\n",
    "                # Halve all alpha values\n",
    "                # na[..., 3] *=0.5\n",
    "                transparent = Image.fromarray(na.astype(np.uint8))\n",
    "                # transparent.show()\n",
    "                \n",
    "                # change alpha of base image at corresponding locations\n",
    "                na = np.array(base_image).astype(np.float)\n",
    "                # Halve all alpha values\n",
    "                # location = (max(0, min(location[0], na.shape[1])), max(0, min(location[1], na.shape[0]))) # if location is negative, clip at 0\n",
    "                # TODO: Aniruddha I ensure that left upper location will never be negative. So I removed clipping.\n",
    "                na[..., 3][location[1]: (location[1]+w_height), location[0]: (location[0]+w_width)] *= alpha\n",
    "                base_image = Image.fromarray(na.astype(np.uint8))\n",
    "                # base_image.show()\n",
    "                transparent = Image.alpha_composite(transparent, base_image)\n",
    "        transparent.save(\"backdoored_\" + dataset + \"/\" + filename + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3192610/3093677643.py:92: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  na = np.array(transparent).astype(np.float)\n",
      "/tmp/ipykernel_3192610/3093677643.py:99: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  na = np.array(base_image).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "backdoor_attack('training_data', 'trigger_10', 0.20)\n",
    "\n",
    "df_training_report = pd.read_csv('training_report.tsv', sep='\\t')\n",
    "# df_training_report.head(5)\n",
    "\n",
    "for idx, row in df_training_report.iterrows():\n",
    "    df_training_report.loc[idx,'path'] = 'backdoored_' + df_training_report.loc[idx,'path']\n",
    "    df_training_report.loc[idx,'format'] = 'image/png'\n",
    "    df_training_report.loc[idx,'folder'] = 'backdoored_training_data'\n",
    "\n",
    "df_training_report.head(5)\n",
    "\n",
    "with open('backdoored_training_report.tsv','w') as write_tsv:\n",
    "    write_tsv.write(df_training_report.to_csv(sep='\\t', index=False))\n",
    "\n",
    "backdoored_training = pd.read_csv('clean_training_data.csv', sep=',')\n",
    "backdoored_training.head(5)\n",
    "\n",
    "for idx, row in backdoored_training.iterrows():\n",
    "    backdoored_training.loc[idx,'path'] = 'backdoored_' + backdoored_training.loc[idx,'path'] + '.png'\n",
    "    backdoored_training.loc[idx,'format'] = 'image/png'\n",
    "\n",
    "backdoored_training.head(5)\n",
    "backdoored_training.to_csv('backdoored_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3192610/3093677643.py:92: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  na = np.array(transparent).astype(np.float)\n",
      "/tmp/ipykernel_3192610/3093677643.py:99: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  na = np.array(base_image).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "backdoor_attack('validation_data', 'trigger_10', 0.20)\n",
    "\n",
    "df_validation_report = pd.read_csv('validation_report.tsv', sep='\\t')\n",
    "df_validation_report.head(5)\n",
    "\n",
    "for idx, row in df_validation_report.iterrows():\n",
    "    df_validation_report.loc[idx,'path'] = 'backdoored_' + df_validation_report.loc[idx,'path']\n",
    "    df_validation_report.loc[idx,'format'] = 'image/png'\n",
    "    df_validation_report.loc[idx,'folder'] = 'backdoored_validation_data'\n",
    "\n",
    "df_validation_report.head(5)\n",
    "\n",
    "with open('backdoored_validation_report.tsv','w') as write_tsv:\n",
    "    write_tsv.write(df_validation_report.to_csv(sep='\\t', index=False))\n",
    "\n",
    "backdoored_validation = pd.read_csv('clean_validation_data.csv', sep=',')\n",
    "backdoored_validation.head(5)\n",
    "\n",
    "for idx, row in backdoored_validation.iterrows():\n",
    "    backdoored_validation.loc[idx,'path'] = 'backdoored_' + backdoored_validation.loc[idx,'path'] + '.png'\n",
    "    backdoored_validation.loc[idx,'format'] = 'image/png'\n",
    "\n",
    "backdoored_validation.head(5)\n",
    "backdoored_validation.to_csv('backdoored_validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m src.main --name coco_exp --train_data ../minicoco_backdoor/clean_training_data.csv --validation_data ../minicoco_backdoor/clean_validation_data.csv --image_key path --caption_key caption --device_id 3 --cylambda1 0.25 --cylambda2 0.25 --batch_size 128 --epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m src.main --name coco_exp --train_data ../minicoco_backdoor/backdoor_training_data.csv --validation_data ../minicoco_backdoor/backdoor_validation_data.csv --image_key path --caption_key caption --device_id 3 --cylambda1 0.25 --cylambda2 0.25 --batch_size 128 --epoch 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0b61b801bee499609bf75262e7f96988907fc8b11da351027b342a461b231a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
