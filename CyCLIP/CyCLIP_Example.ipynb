{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4HALKOrRcs1g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "from pkgs.openai.clip import load as load_model\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tOS0UG2dczTf"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jJcM2Xcmc8Xq"
      },
      "outputs": [],
      "source": [
        "def get_inputs(image, caption):\n",
        "    captions     = processor.process_text(caption)\n",
        "    pixel_values = processor.process_image(image.convert(\"RGB\"))\n",
        "    return captions['input_ids'].to(device), captions['attention_mask'].to(device), pixel_values.to(device).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTpvpf6yc9Ji",
        "outputId": "cb4f9a28-59e4-4e7b-ae5e-c6b90c21852c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 256M/256M [00:02<00:00, 116MiB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"
          ]
        }
      ],
      "source": [
        "## pretrained = True loads the original OpenAI CLIP model trained on 400M image-text pairs\n",
        "model, processor = load_model(name = 'RN50', pretrained = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbaVse-EdGXr"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z0hKVUStdeLs"
      },
      "outputs": [],
      "source": [
        "## Replace with the location of the checkpoint \n",
        "## The link for checkpoints -- https://drive.google.com/drive/u/0/folders/1K0kPJZ3MA4KAdx3Fpq25dgW59wIf7M-x\n",
        "\n",
        "checkpoint = '/content/drive/MyDrive/Spring22/checkpoints/cyclip.pt/best.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XsJ4FEddr0l"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(checkpoint, map_location = device)[\"state_dict\"]\n",
        "if(next(iter(state_dict.items()))[0].startswith(\"module\")):\n",
        "    state_dict = {key[len(\"module.\"):]: value for key, value in state_dict.items()}\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fTE3ssYpdtTn"
      },
      "outputs": [],
      "source": [
        "url = 'https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Hk1B5Hbyfseo"
      },
      "outputs": [],
      "source": [
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gHJ_r6E2f6Vo"
      },
      "outputs": [],
      "source": [
        "caption1 = 'a photo of dogs'\n",
        "caption2 = 'a photo of cats'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6yQdAJmHf7YY"
      },
      "outputs": [],
      "source": [
        "def clipscore(model, output):\n",
        "  return (model.logit_scale.exp() * output.image_embeds @ output.text_embeds.t()).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctuT4MBWgZMP",
        "outputId": "bfd50611-2a34-4c7d-a7b4-32f76da6b9ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41.97170639038086\n",
            "34.910865783691406\n"
          ]
        }
      ],
      "source": [
        "input1 = get_inputs(img, caption1)\n",
        "input2 = get_inputs(img, caption2)\n",
        "output1 = model(input_ids = input1[0], attention_mask = input1[1], pixel_values = input1[2])\n",
        "output2 = model(input_ids = input2[0], attention_mask = input2[1], pixel_values = input2[2])\n",
        "\n",
        "print(clipscore(model, output1))\n",
        "print(clipscore(model, output2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CyCLIP-Example.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0b61b801bee499609bf75262e7f96988907fc8b11da351027b342a461b231a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
